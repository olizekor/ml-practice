{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-26T18:28:36.180957Z","iopub.execute_input":"2025-02-26T18:28:36.181327Z","iopub.status.idle":"2025-02-26T18:28:36.518948Z","shell.execute_reply.started":"2025-02-26T18:28:36.181298Z","shell.execute_reply":"2025-02-26T18:28:36.518032Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/digit-recognizer/sample_submission.csv\n/kaggle/input/digit-recognizer/train.csv\n/kaggle/input/digit-recognizer/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"#Deep learning libraries\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom torchvision.utils import make_grid\n\nimport math\nimport random\n\nfrom PIL import Image, ImageOps, ImageEnhance\nimport numbers\n\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T18:28:37.862320Z","iopub.execute_input":"2025-02-26T18:28:37.862772Z","iopub.status.idle":"2025-02-26T18:28:40.260236Z","shell.execute_reply.started":"2025-02-26T18:28:37.862744Z","shell.execute_reply":"2025-02-26T18:28:40.259444Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"#set random seeds\nnp.random.seed(27)\ntorch.manual_seed(27)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T18:28:46.279567Z","iopub.execute_input":"2025-02-26T18:28:46.280018Z","iopub.status.idle":"2025-02-26T18:28:46.287386Z","shell.execute_reply.started":"2025-02-26T18:28:46.279998Z","shell.execute_reply":"2025-02-26T18:28:46.286396Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x78d805bca110>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Get device for training\ndevice = (\n    \"cuda\"\n    if torch.cuda.is_available()\n    else \"mps\"\n    if torch.backends.mps.is_available()\n    else \"cpu\"\n)\nprint(f\"Using {device} device\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T18:28:47.550675Z","iopub.execute_input":"2025-02-26T18:28:47.551002Z","iopub.status.idle":"2025-02-26T18:28:47.555752Z","shell.execute_reply.started":"2025-02-26T18:28:47.550978Z","shell.execute_reply":"2025-02-26T18:28:47.554933Z"}},"outputs":[{"name":"stdout","text":"Using cpu device\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"#loading the data\n\nclass MNIST_data(Dataset):\n    def __init__(self, filepath, transform = transforms.Compose([transforms.ToTensor,\n                                                               transforms.Normalize(mean=(0.5,), std=(0.5,))])):\n        #data loading\n        df = pd.read_csv(filepath)\n        if len(df.columns) == 784:\n            self.X = df.values.reshape(-1,28,28).astype(np.float32)[:,None,:,:]\n            self.y = None\n        else:\n            self.X = df.iloc[:,1:].values.reshape(-1,28,28).astype(np.float32)[:,None,:,:]\n            self.y = torch.from_numpy(df.iloc[:,0].values)\n        self.n_samples = df.shape[0]\n        self.transform = transform\n        \n    def __getitem__(self, index):\n        #dataset indexing.\n        if self.y is not None:\n            return self.X[index], self.y[index]\n        else:\n            return self.X[index]\n        \n    def __len__(self):\n        #len(dataset)\n        return len(self.X)\n\n    def TrainValSplit(self, split):\n        split_idx = int(self.n_samples*split)\n        return torch.utils.data.random_split(self, [split_idx, self.n_samples - split_idx])\n\ntrain_dataset = MNIST_data(\"/kaggle/input/digit-recognizer/train.csv\")\ntest_dataset = MNIST_data(\"/kaggle/input/digit-recognizer/test.csv\")\n\nt_data, v_data = train_dataset.TrainValSplit(0.8)\n\n#testing code\nfeatures, labels = train_dataset[0]\nprint(features.shape)\nprint(labels)\nprint(labels.ndimension())\nprint(len(v_data))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T18:28:49.077142Z","iopub.execute_input":"2025-02-26T18:28:49.077473Z","iopub.status.idle":"2025-02-26T18:28:51.234346Z","shell.execute_reply.started":"2025-02-26T18:28:49.077448Z","shell.execute_reply":"2025-02-26T18:28:51.233689Z"}},"outputs":[{"name":"stdout","text":"(1, 28, 28)\ntensor(1)\n0\n8400\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# visualise some examples\nimport matplotlib.pyplot as plt\n\n# Assuming `training_data` is already defined as a PyTorch dataset\nfigure = plt.figure(figsize=(8, 8))\ncols, rows = 3, 3\n\nfor i in range(1, cols * rows + 1):\n    sample_idx = torch.randint(100, size=(1,)).item()\n    img, label = train_dataset[sample_idx]\n    \n    figure.add_subplot(rows, cols, i)\n    plt.title(str(label)[7])\n    plt.axis(\"off\")\n    plt.imshow(img.reshape(28, 28), cmap=\"gray\")  \n    \nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T18:28:55.644385Z","iopub.execute_input":"2025-02-26T18:28:55.644730Z","iopub.status.idle":"2025-02-26T18:28:56.116717Z","shell.execute_reply.started":"2025-02-26T18:28:55.644707Z","shell.execute_reply":"2025-02-26T18:28:56.115848Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 800x800 with 9 Axes>","image/png":"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"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# calling the data loader\ntrain_dataloader = DataLoader(t_data, batch_size = 1024, shuffle = True)\nvalid_dataloader = DataLoader(v_data, batch_size = 1024, shuffle = True)\ntest_dataloader = DataLoader(test_dataset, batch_size = 1024, shuffle = False)\n\nprint(valid_dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T18:28:59.598422Z","iopub.execute_input":"2025-02-26T18:28:59.598750Z","iopub.status.idle":"2025-02-26T18:28:59.604270Z","shell.execute_reply.started":"2025-02-26T18:28:59.598728Z","shell.execute_reply":"2025-02-26T18:28:59.603307Z"}},"outputs":[{"name":"stdout","text":"<torch.utils.data.dataloader.DataLoader object at 0x78d7f3c8feb0>\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"train_features, train_labels = next(iter(train_dataloader))\nprint(f\"Feature batch shape: {train_features.size()}\")\nprint(f\"Labels batch shape: {train_labels.size()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T18:29:01.328649Z","iopub.execute_input":"2025-02-26T18:29:01.328970Z","iopub.status.idle":"2025-02-26T18:29:01.355002Z","shell.execute_reply.started":"2025-02-26T18:29:01.328947Z","shell.execute_reply":"2025-02-26T18:29:01.353979Z"}},"outputs":[{"name":"stdout","text":"Feature batch shape: torch.Size([1024, 1, 28, 28])\nLabels batch shape: torch.Size([1024])\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# structure of the neural network\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        #self.flatten = nn.Flatten()\n        self.relu_layer = nn.ReLU()\n        self.dense_1 = nn.Linear(28*28,20)\n        self.dense_2 = nn.Linear(20,15)\n        self.dense_output = nn.Linear(15,10)\n\n    def forward(self, x):\n        x = self.relu_layer(self.dense_1(x))\n        x = self.relu_layer(self.dense_2(x))\n        logits = self.dense_output(x); return logits\nprint(NeuralNetwork)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T18:29:03.662589Z","iopub.execute_input":"2025-02-26T18:29:03.662888Z","iopub.status.idle":"2025-02-26T18:29:03.668283Z","shell.execute_reply.started":"2025-02-26T18:29:03.662865Z","shell.execute_reply":"2025-02-26T18:29:03.667564Z"}},"outputs":[{"name":"stdout","text":"<class '__main__.NeuralNetwork'>\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# structure of the convolutional neural network\nclass CNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n        self.conv_1_batchnorm = nn.BatchNorm2d(16)\n        self.conv_2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n        self.conv_2_batchnorm = nn.BatchNorm2d(8)\n        self.dense_1 = nn.Linear(8*7*7, 20)\n        self.dense_output = nn.Linear(20, 10)\n\n    def forward(self, x):\n        x = self.conv_1_batchnorm(self.conv_1(x))\n        x = F.max_pool2d(torch.relu(x), 2)\n        x = self.conv_2_batchnorm(self.conv_2(x))\n        x = F.max_pool2d(torch.relu(x), 2)\n        x = x.view(-1, 8*7*7)\n        x = torch.relu(self.dense_1(x))\n        logits = self.dense_output(x)\n        return logits\nprint(CNN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T18:38:00.140995Z","iopub.execute_input":"2025-02-26T18:38:00.141336Z","iopub.status.idle":"2025-02-26T18:38:00.148440Z","shell.execute_reply.started":"2025-02-26T18:38:00.141311Z","shell.execute_reply":"2025-02-26T18:38:00.147656Z"}},"outputs":[{"name":"stdout","text":"<class '__main__.CNN'>\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"# model hyperparameters\nepochs = 50\nbatch_size = 1024\nlearning_rate = 0.001","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T18:38:01.628542Z","iopub.execute_input":"2025-02-26T18:38:01.629435Z","iopub.status.idle":"2025-02-26T18:38:01.632774Z","shell.execute_reply.started":"2025-02-26T18:38:01.629402Z","shell.execute_reply":"2025-02-26T18:38:01.632034Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"# instance of the NN\nmodel = CNN()\n\nmodel.to(device)\n\noptimizer = optim.Adam(model.parameters(), lr = learning_rate)\n\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2025-02-26T18:38:01.852171Z","iopub.execute_input":"2025-02-26T18:38:01.852473Z","iopub.status.idle":"2025-02-26T18:38:01.859886Z","shell.execute_reply.started":"2025-02-26T18:38:01.852454Z","shell.execute_reply":"2025-02-26T18:38:01.858967Z"},"trusted":true},"outputs":[{"name":"stdout","text":"CNN(\n  (conv_1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv_1_batchnorm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv_2): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv_2_batchnorm): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dense_1): Linear(in_features=392, out_features=20, bias=True)\n  (dense_output): Linear(in_features=20, out_features=10, bias=True)\n)\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"# define cel and accuracy\ndef cel(y_true, y_pred):\n    y_true = y_true.long().squeeze()\n    return nn.CrossEntropyLoss()(y_pred, y_true)\n\ndef acc(y_true, y_pred):\n    y_true = y_true.long().squeeze()\n    y_pred = torch.argmax(y_pred, axis=1)\n    return (y_true == y_pred).float().sum()/len(y_true)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T18:38:02.038604Z","iopub.execute_input":"2025-02-26T18:38:02.038933Z","iopub.status.idle":"2025-02-26T18:38:02.044734Z","shell.execute_reply.started":"2025-02-26T18:38:02.038908Z","shell.execute_reply":"2025-02-26T18:38:02.043731Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"#training loop\ntrain_losses, valid_losses = [], []\ntrain_accuracies, valid_accuracies = [], []\n\nfor epoch in range(epochs):\n    model.train()\n    print(f\"Epoch: {epoch + 1}\")\n    batch_train_losses, batch_train_accuracies = [], []\n\n    batch = 0\n    for train_batch in train_dataloader:\n        train_X, train_y = train_batch\n        train_preds = model(train_X)\n        train_loss = cel(train_y, train_preds)\n        train_accuracy = acc(train_y, train_preds)\n\n        optimizer.zero_grad()\n        train_loss.backward()\n\n        optimizer.step()\n        train_loss = np.round(train_loss.item(), 3)\n        train_accuracy = np.round(train_accuracy.item(), 3)\n\n        batch += 1\n        log = batch % 10 == 0\n\n        batch_train_losses.append(train_loss)\n        batch_train_accuracies.append(train_accuracy)\n\n        if log: print(f\"\"\"Batch: {batch} || Train Loss: {train_loss} \n                        || Train Acc: {train_accuracy}\"\"\")\n\n    train_losses.append(np.mean(batch_train_losses))\n    train_accuracies.append(np.mean(batch_train_accuracies))\n\n    total_valid_loss, total_valid_points, total_valid_accuracy = 0, 0, 0\n\n    with torch.no_grad():\n        for valid_batch in valid_dataloader:\n            valid_X, valid_y = valid_batch\n\n            valid_preds = model.forward(valid_X)\n            valid_loss = cel(valid_y, valid_preds)\n            valid_accuracy = acc(valid_y, valid_preds)\n\n            total_valid_points += 1\n            total_valid_loss += valid_loss.item()\n            total_valid_accuracy += valid_accuracy.item()\n\n        valid_loss = np.round(total_valid_loss/total_valid_points, 3)\n        valid_accuracy = np.round(total_valid_accuracy/total_valid_points, 3)\n\n        valid_losses.append(valid_loss)\n        valid_accuracies.append(valid_accuracy)\n        \n        print(f\"\"\"Epoch: {epoch + 1} || Valid Loss: {valid_loss} || Valid Acc: {valid_accuracy}\"\"\")\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T18:38:04.980570Z","iopub.execute_input":"2025-02-26T18:38:04.980861Z","iopub.status.idle":"2025-02-26T18:47:43.703188Z","shell.execute_reply.started":"2025-02-26T18:38:04.980842Z","shell.execute_reply":"2025-02-26T18:47:43.702545Z"}},"outputs":[{"name":"stdout","text":"Epoch: 1\nBatch: 10 || Train Loss: 1.808 \n                        || Train Acc: 0.519\nBatch: 20 || Train Loss: 1.345 \n                        || Train Acc: 0.673\nBatch: 30 || Train Loss: 0.979 \n                        || Train Acc: 0.798\nEpoch: 1 || Valid Loss: 0.831 || Valid Acc: 0.836\nEpoch: 2\nBatch: 10 || Train Loss: 0.618 \n                        || Train Acc: 0.861\nBatch: 20 || Train Loss: 0.475 \n                        || Train Acc: 0.887\nBatch: 30 || Train Loss: 0.363 \n                        || Train Acc: 0.908\nEpoch: 2 || Valid Loss: 0.358 || Valid Acc: 0.909\nEpoch: 3\nBatch: 10 || Train Loss: 0.32 \n                        || Train Acc: 0.916\nBatch: 20 || Train Loss: 0.252 \n                        || Train Acc: 0.932\nBatch: 30 || Train Loss: 0.222 \n                        || Train Acc: 0.951\nEpoch: 3 || Valid Loss: 0.236 || Valid Acc: 0.938\nEpoch: 4\nBatch: 10 || Train Loss: 0.211 \n                        || Train Acc: 0.949\nBatch: 20 || Train Loss: 0.189 \n                        || Train Acc: 0.956\nBatch: 30 || Train Loss: 0.17 \n                        || Train Acc: 0.953\nEpoch: 4 || Valid Loss: 0.175 || Valid Acc: 0.955\nEpoch: 5\nBatch: 10 || Train Loss: 0.139 \n                        || Train Acc: 0.957\nBatch: 20 || Train Loss: 0.15 \n                        || Train Acc: 0.964\nBatch: 30 || Train Loss: 0.128 \n                        || Train Acc: 0.962\nEpoch: 5 || Valid Loss: 0.149 || Valid Acc: 0.96\nEpoch: 6\nBatch: 10 || Train Loss: 0.141 \n                        || Train Acc: 0.966\nBatch: 20 || Train Loss: 0.115 \n                        || Train Acc: 0.97\nBatch: 30 || Train Loss: 0.113 \n                        || Train Acc: 0.969\nEpoch: 6 || Valid Loss: 0.125 || Valid Acc: 0.963\nEpoch: 7\nBatch: 10 || Train Loss: 0.09 \n                        || Train Acc: 0.976\nBatch: 20 || Train Loss: 0.088 \n                        || Train Acc: 0.981\nBatch: 30 || Train Loss: 0.091 \n                        || Train Acc: 0.977\nEpoch: 7 || Valid Loss: 0.115 || Valid Acc: 0.966\nEpoch: 8\nBatch: 10 || Train Loss: 0.098 \n                        || Train Acc: 0.978\nBatch: 20 || Train Loss: 0.088 \n                        || Train Acc: 0.975\nBatch: 30 || Train Loss: 0.093 \n                        || Train Acc: 0.981\nEpoch: 8 || Valid Loss: 0.105 || Valid Acc: 0.969\nEpoch: 9\nBatch: 10 || Train Loss: 0.082 \n                        || Train Acc: 0.977\nBatch: 20 || Train Loss: 0.056 \n                        || Train Acc: 0.985\nBatch: 30 || Train Loss: 0.095 \n                        || Train Acc: 0.973\nEpoch: 9 || Valid Loss: 0.096 || Valid Acc: 0.971\nEpoch: 10\nBatch: 10 || Train Loss: 0.081 \n                        || Train Acc: 0.98\nBatch: 20 || Train Loss: 0.075 \n                        || Train Acc: 0.979\nBatch: 30 || Train Loss: 0.075 \n                        || Train Acc: 0.982\nEpoch: 10 || Valid Loss: 0.095 || Valid Acc: 0.972\nEpoch: 11\nBatch: 10 || Train Loss: 0.052 \n                        || Train Acc: 0.986\nBatch: 20 || Train Loss: 0.043 \n                        || Train Acc: 0.986\nBatch: 30 || Train Loss: 0.065 \n                        || Train Acc: 0.98\nEpoch: 11 || Valid Loss: 0.085 || Valid Acc: 0.975\nEpoch: 12\nBatch: 10 || Train Loss: 0.048 \n                        || Train Acc: 0.986\nBatch: 20 || Train Loss: 0.065 \n                        || Train Acc: 0.979\nBatch: 30 || Train Loss: 0.06 \n                        || Train Acc: 0.984\nEpoch: 12 || Valid Loss: 0.082 || Valid Acc: 0.976\nEpoch: 13\nBatch: 10 || Train Loss: 0.054 \n                        || Train Acc: 0.985\nBatch: 20 || Train Loss: 0.038 \n                        || Train Acc: 0.99\nBatch: 30 || Train Loss: 0.05 \n                        || Train Acc: 0.982\nEpoch: 13 || Valid Loss: 0.082 || Valid Acc: 0.975\nEpoch: 14\nBatch: 10 || Train Loss: 0.035 \n                        || Train Acc: 0.991\nBatch: 20 || Train Loss: 0.048 \n                        || Train Acc: 0.987\nBatch: 30 || Train Loss: 0.045 \n                        || Train Acc: 0.986\nEpoch: 14 || Valid Loss: 0.076 || Valid Acc: 0.978\nEpoch: 15\nBatch: 10 || Train Loss: 0.05 \n                        || Train Acc: 0.985\nBatch: 20 || Train Loss: 0.039 \n                        || Train Acc: 0.99\nBatch: 30 || Train Loss: 0.049 \n                        || Train Acc: 0.982\nEpoch: 15 || Valid Loss: 0.075 || Valid Acc: 0.979\nEpoch: 16\nBatch: 10 || Train Loss: 0.052 \n                        || Train Acc: 0.986\nBatch: 20 || Train Loss: 0.049 \n                        || Train Acc: 0.982\nBatch: 30 || Train Loss: 0.037 \n                        || Train Acc: 0.988\nEpoch: 16 || Valid Loss: 0.079 || Valid Acc: 0.977\nEpoch: 17\nBatch: 10 || Train Loss: 0.036 \n                        || Train Acc: 0.99\nBatch: 20 || Train Loss: 0.036 \n                        || Train Acc: 0.991\nBatch: 30 || Train Loss: 0.033 \n                        || Train Acc: 0.994\nEpoch: 17 || Valid Loss: 0.073 || Valid Acc: 0.978\nEpoch: 18\nBatch: 10 || Train Loss: 0.031 \n                        || Train Acc: 0.992\nBatch: 20 || Train Loss: 0.034 \n                        || Train Acc: 0.992\nBatch: 30 || Train Loss: 0.035 \n                        || Train Acc: 0.992\nEpoch: 18 || Valid Loss: 0.069 || Valid Acc: 0.98\nEpoch: 19\nBatch: 10 || Train Loss: 0.046 \n                        || Train Acc: 0.986\nBatch: 20 || Train Loss: 0.028 \n                        || Train Acc: 0.995\nBatch: 30 || Train Loss: 0.021 \n                        || Train Acc: 0.996\nEpoch: 19 || Valid Loss: 0.071 || Valid Acc: 0.979\nEpoch: 20\nBatch: 10 || Train Loss: 0.038 \n                        || Train Acc: 0.989\nBatch: 20 || Train Loss: 0.03 \n                        || Train Acc: 0.992\nBatch: 30 || Train Loss: 0.036 \n                        || Train Acc: 0.988\nEpoch: 20 || Valid Loss: 0.075 || Valid Acc: 0.978\nEpoch: 21\nBatch: 10 || Train Loss: 0.027 \n                        || Train Acc: 0.995\nBatch: 20 || Train Loss: 0.028 \n                        || Train Acc: 0.993\nBatch: 30 || Train Loss: 0.026 \n                        || Train Acc: 0.992\nEpoch: 21 || Valid Loss: 0.066 || Valid Acc: 0.981\nEpoch: 22\nBatch: 10 || Train Loss: 0.027 \n                        || Train Acc: 0.993\nBatch: 20 || Train Loss: 0.031 \n                        || Train Acc: 0.993\nBatch: 30 || Train Loss: 0.031 \n                        || Train Acc: 0.991\nEpoch: 22 || Valid Loss: 0.066 || Valid Acc: 0.98\nEpoch: 23\nBatch: 10 || Train Loss: 0.034 \n                        || Train Acc: 0.99\nBatch: 20 || Train Loss: 0.038 \n                        || Train Acc: 0.99\nBatch: 30 || Train Loss: 0.028 \n                        || Train Acc: 0.991\nEpoch: 23 || Valid Loss: 0.064 || Valid Acc: 0.983\nEpoch: 24\nBatch: 10 || Train Loss: 0.031 \n                        || Train Acc: 0.991\nBatch: 20 || Train Loss: 0.03 \n                        || Train Acc: 0.987\nBatch: 30 || Train Loss: 0.028 \n                        || Train Acc: 0.991\nEpoch: 24 || Valid Loss: 0.066 || Valid Acc: 0.981\nEpoch: 25\nBatch: 10 || Train Loss: 0.023 \n                        || Train Acc: 0.994\nBatch: 20 || Train Loss: 0.025 \n                        || Train Acc: 0.992\nBatch: 30 || Train Loss: 0.028 \n                        || Train Acc: 0.994\nEpoch: 25 || Valid Loss: 0.061 || Valid Acc: 0.982\nEpoch: 26\nBatch: 10 || Train Loss: 0.038 \n                        || Train Acc: 0.987\nBatch: 20 || Train Loss: 0.028 \n                        || Train Acc: 0.99\nBatch: 30 || Train Loss: 0.021 \n                        || Train Acc: 0.993\nEpoch: 26 || Valid Loss: 0.067 || Valid Acc: 0.98\nEpoch: 27\nBatch: 10 || Train Loss: 0.035 \n                        || Train Acc: 0.991\nBatch: 20 || Train Loss: 0.025 \n                        || Train Acc: 0.993\nBatch: 30 || Train Loss: 0.024 \n                        || Train Acc: 0.991\nEpoch: 27 || Valid Loss: 0.066 || Valid Acc: 0.982\nEpoch: 28\nBatch: 10 || Train Loss: 0.021 \n                        || Train Acc: 0.993\nBatch: 20 || Train Loss: 0.029 \n                        || Train Acc: 0.993\nBatch: 30 || Train Loss: 0.032 \n                        || Train Acc: 0.992\nEpoch: 28 || Valid Loss: 0.068 || Valid Acc: 0.98\nEpoch: 29\nBatch: 10 || Train Loss: 0.021 \n                        || Train Acc: 0.994\nBatch: 20 || Train Loss: 0.017 \n                        || Train Acc: 0.998\nBatch: 30 || Train Loss: 0.017 \n                        || Train Acc: 0.997\nEpoch: 29 || Valid Loss: 0.065 || Valid Acc: 0.981\nEpoch: 30\nBatch: 10 || Train Loss: 0.024 \n                        || Train Acc: 0.993\nBatch: 20 || Train Loss: 0.018 \n                        || Train Acc: 0.996\nBatch: 30 || Train Loss: 0.019 \n                        || Train Acc: 0.996\nEpoch: 30 || Valid Loss: 0.065 || Valid Acc: 0.982\nEpoch: 31\nBatch: 10 || Train Loss: 0.019 \n                        || Train Acc: 0.996\nBatch: 20 || Train Loss: 0.016 \n                        || Train Acc: 0.996\nBatch: 30 || Train Loss: 0.019 \n                        || Train Acc: 0.995\nEpoch: 31 || Valid Loss: 0.061 || Valid Acc: 0.983\nEpoch: 32\nBatch: 10 || Train Loss: 0.015 \n                        || Train Acc: 0.999\nBatch: 20 || Train Loss: 0.017 \n                        || Train Acc: 0.996\nBatch: 30 || Train Loss: 0.014 \n                        || Train Acc: 0.995\nEpoch: 32 || Valid Loss: 0.064 || Valid Acc: 0.981\nEpoch: 33\nBatch: 10 || Train Loss: 0.016 \n                        || Train Acc: 0.996\nBatch: 20 || Train Loss: 0.02 \n                        || Train Acc: 0.994\nBatch: 30 || Train Loss: 0.021 \n                        || Train Acc: 0.991\nEpoch: 33 || Valid Loss: 0.061 || Valid Acc: 0.983\nEpoch: 34\nBatch: 10 || Train Loss: 0.009 \n                        || Train Acc: 0.999\nBatch: 20 || Train Loss: 0.014 \n                        || Train Acc: 0.999\nBatch: 30 || Train Loss: 0.023 \n                        || Train Acc: 0.994\nEpoch: 34 || Valid Loss: 0.06 || Valid Acc: 0.983\nEpoch: 35\nBatch: 10 || Train Loss: 0.024 \n                        || Train Acc: 0.995\nBatch: 20 || Train Loss: 0.021 \n                        || Train Acc: 0.993\nBatch: 30 || Train Loss: 0.018 \n                        || Train Acc: 0.996\nEpoch: 35 || Valid Loss: 0.068 || Valid Acc: 0.98\nEpoch: 36\nBatch: 10 || Train Loss: 0.022 \n                        || Train Acc: 0.996\nBatch: 20 || Train Loss: 0.021 \n                        || Train Acc: 0.993\nBatch: 30 || Train Loss: 0.013 \n                        || Train Acc: 0.998\nEpoch: 36 || Valid Loss: 0.062 || Valid Acc: 0.982\nEpoch: 37\nBatch: 10 || Train Loss: 0.017 \n                        || Train Acc: 0.995\nBatch: 20 || Train Loss: 0.014 \n                        || Train Acc: 0.996\nBatch: 30 || Train Loss: 0.011 \n                        || Train Acc: 0.998\nEpoch: 37 || Valid Loss: 0.063 || Valid Acc: 0.983\nEpoch: 38\nBatch: 10 || Train Loss: 0.021 \n                        || Train Acc: 0.993\nBatch: 20 || Train Loss: 0.026 \n                        || Train Acc: 0.993\nBatch: 30 || Train Loss: 0.016 \n                        || Train Acc: 0.995\nEpoch: 38 || Valid Loss: 0.059 || Valid Acc: 0.984\nEpoch: 39\nBatch: 10 || Train Loss: 0.017 \n                        || Train Acc: 0.996\nBatch: 20 || Train Loss: 0.016 \n                        || Train Acc: 0.999\nBatch: 30 || Train Loss: 0.015 \n                        || Train Acc: 0.996\nEpoch: 39 || Valid Loss: 0.067 || Valid Acc: 0.981\nEpoch: 40\nBatch: 10 || Train Loss: 0.015 \n                        || Train Acc: 0.996\nBatch: 20 || Train Loss: 0.027 \n                        || Train Acc: 0.996\nBatch: 30 || Train Loss: 0.014 \n                        || Train Acc: 0.997\nEpoch: 40 || Valid Loss: 0.063 || Valid Acc: 0.982\nEpoch: 41\nBatch: 10 || Train Loss: 0.025 \n                        || Train Acc: 0.998\nBatch: 20 || Train Loss: 0.007 \n                        || Train Acc: 0.999\nBatch: 30 || Train Loss: 0.013 \n                        || Train Acc: 0.997\nEpoch: 41 || Valid Loss: 0.066 || Valid Acc: 0.983\nEpoch: 42\nBatch: 10 || Train Loss: 0.007 \n                        || Train Acc: 0.999\nBatch: 20 || Train Loss: 0.011 \n                        || Train Acc: 0.997\nBatch: 30 || Train Loss: 0.018 \n                        || Train Acc: 0.996\nEpoch: 42 || Valid Loss: 0.066 || Valid Acc: 0.982\nEpoch: 43\nBatch: 10 || Train Loss: 0.01 \n                        || Train Acc: 0.997\nBatch: 20 || Train Loss: 0.01 \n                        || Train Acc: 0.997\nBatch: 30 || Train Loss: 0.01 \n                        || Train Acc: 0.999\nEpoch: 43 || Valid Loss: 0.06 || Valid Acc: 0.983\nEpoch: 44\nBatch: 10 || Train Loss: 0.008 \n                        || Train Acc: 0.999\nBatch: 20 || Train Loss: 0.01 \n                        || Train Acc: 1.0\nBatch: 30 || Train Loss: 0.009 \n                        || Train Acc: 1.0\nEpoch: 44 || Valid Loss: 0.061 || Valid Acc: 0.983\nEpoch: 45\nBatch: 10 || Train Loss: 0.008 \n                        || Train Acc: 1.0\nBatch: 20 || Train Loss: 0.008 \n                        || Train Acc: 0.999\nBatch: 30 || Train Loss: 0.013 \n                        || Train Acc: 0.996\nEpoch: 45 || Valid Loss: 0.07 || Valid Acc: 0.981\nEpoch: 46\nBatch: 10 || Train Loss: 0.018 \n                        || Train Acc: 0.996\nBatch: 20 || Train Loss: 0.015 \n                        || Train Acc: 0.996\nBatch: 30 || Train Loss: 0.014 \n                        || Train Acc: 0.996\nEpoch: 46 || Valid Loss: 0.061 || Valid Acc: 0.983\nEpoch: 47\nBatch: 10 || Train Loss: 0.007 \n                        || Train Acc: 0.999\nBatch: 20 || Train Loss: 0.014 \n                        || Train Acc: 0.996\nBatch: 30 || Train Loss: 0.008 \n                        || Train Acc: 0.999\nEpoch: 47 || Valid Loss: 0.067 || Valid Acc: 0.982\nEpoch: 48\nBatch: 10 || Train Loss: 0.024 \n                        || Train Acc: 0.999\nBatch: 20 || Train Loss: 0.007 \n                        || Train Acc: 0.999\nBatch: 30 || Train Loss: 0.009 \n                        || Train Acc: 0.998\nEpoch: 48 || Valid Loss: 0.063 || Valid Acc: 0.982\nEpoch: 49\nBatch: 10 || Train Loss: 0.01 \n                        || Train Acc: 0.998\nBatch: 20 || Train Loss: 0.012 \n                        || Train Acc: 0.997\nBatch: 30 || Train Loss: 0.011 \n                        || Train Acc: 0.997\nEpoch: 49 || Valid Loss: 0.059 || Valid Acc: 0.984\nEpoch: 50\nBatch: 10 || Train Loss: 0.011 \n                        || Train Acc: 0.997\nBatch: 20 || Train Loss: 0.008 \n                        || Train Acc: 0.999\nBatch: 30 || Train Loss: 0.023 \n                        || Train Acc: 0.999\nEpoch: 50 || Valid Loss: 0.063 || Valid Acc: 0.983\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"#prediction on test set\ndef softmax(x):\n    return np.exp(x)/np.sum(np.exp(x), axis=1)[:,None]\n\ntest_preds = []\nwith torch.no_grad():\n    for test_X in test_dataloader:\n        test_pred = model.forward(test_X)\n        test_preds.append(softmax(test_pred.detach().numpy()))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T18:47:49.847346Z","iopub.execute_input":"2025-02-26T18:47:49.847679Z","iopub.status.idle":"2025-02-26T18:47:53.991650Z","shell.execute_reply.started":"2025-02-26T18:47:49.847655Z","shell.execute_reply":"2025-02-26T18:47:53.990906Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"submission = pd.read_csv(\"../input/digit-recognizer/sample_submission.csv\")\nsubmission[\"Label\"] = np.argmax(np.concatenate(test_preds, axis=0), axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T18:47:56.630142Z","iopub.execute_input":"2025-02-26T18:47:56.630932Z","iopub.status.idle":"2025-02-26T18:47:56.651737Z","shell.execute_reply.started":"2025-02-26T18:47:56.630903Z","shell.execute_reply":"2025-02-26T18:47:56.650761Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T18:47:59.606096Z","iopub.execute_input":"2025-02-26T18:47:59.606417Z","iopub.status.idle":"2025-02-26T18:47:59.623749Z","shell.execute_reply.started":"2025-02-26T18:47:59.606390Z","shell.execute_reply":"2025-02-26T18:47:59.622562Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"submission.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T18:48:02.740022Z","iopub.execute_input":"2025-02-26T18:48:02.740349Z","iopub.status.idle":"2025-02-26T18:48:02.758653Z","shell.execute_reply.started":"2025-02-26T18:48:02.740327Z","shell.execute_reply":"2025-02-26T18:48:02.757716Z"}},"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"   ImageId  Label\n0        1      2\n1        2      0\n2        3      9\n3        4      0\n4        5      3","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ImageId</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":49},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}